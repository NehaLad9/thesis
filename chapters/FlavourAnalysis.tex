\setchapterpreamble[u]{\margintoc}
\chapter{Flavour Composition Analysis}
\labch{analysis}
Using the High Energy Starting Event (HESE) sample introduced in \refch{nu_samples}, that classifies high-energy neutrino events into one of three distinct morphologies (Single Cascades, Tracks and Double Cascades), a flavour measurement of the high energy diffuse neutrino spectrum can be performed. This chapter focuses on the methods and various ingredients used to make this flavour measurement. 

The chapter begins by detailing these statistical techniques, with a focus on forward folding likelihood fits, which plays a crucial role in distinguishing signal from background in measuring the flavour composition of the astrophysical neutrino flux. The modelling of signal and background events, enhanced for the iteration of this analysis performed in this thesis are explained subsequently. Finally, the chapter evaluates the sensitivity of the analysis, providing estimates of detection limits.
\section{Statistical Methods}
\label{sec:analysis}
This analysis utilizes a binned maximum likelihood estimation (MLE) approach to perform statistical inference based on the observed data. Binning the data offers computational efficiency as computing probability densities for an unbinned likelihood is not trivial in the case of finite MC statistics. One need to somehow parametrize them as a function of the signal and nuisance parameters. Forward-folding circumvents this by directly re-computing the distributions corresponding to a certain parameter set. Each analysis bin contains data points from a specific range of measured quantities, such as energy or angular distributions. To incorporate the complexities of detector response and model uncertainties, the concept of forward folding is employed. Forward folding integrates theoretical model predictions with the response of the detector, producing distributions of the expected event rate that can be directly compared to the binned observed data. This method allows for testing different flux models of arbitrary shape, along with the systematic uncertainties of these models as well as the detector responses, all folded into the predicted likelihood. 

\subsection{Binned Maximum-likelihood Fits}
\label{sec:MLE}
When fitting a data sample with a large size (large $N$), it is common practice to bin the data in order to enhance the computational efficiency of the likelihood function calculation. This binning procedure groups the data into discrete intervals, or \emph{bins}, based on one or more \emph{observables} (derived from reconstructed properties of the neutrino events), such as energy or zenith. This simplification is not harmful as long as the variation of the probability density function (PDF), $f(x|\theta)$, within each bin is insignificant compared to its variation across the neighbouring bins. In other words, the binning should not discard crucial information about the parameter vector $\theta$ that governs the underlying physical model. If a binning is applied appropriately, the fit retains the ability to accurately infer the parameters of interest while significantly speeding up the computational process, particularly in high-dimensional analyses where unbinned likelihood fits may be computationally prohibitive.

The binned likelihood method works by comparing the number of observed events in each bin to the expected number of events, which are typically predicted through Monte Carlo simulations. In the analysis, these Monte Carlo templates are created for various theoretical flux components, such as astrophysical, atmospheric etc. that constitute signal and background to the measurement. Each template provides a predicted event distribution based on specific values of the \emph{signal} parameters $\theta$ along with \emph{nuisance} parameters, $\xi$, which describe the physical processes being studied. These templates are obtained from forward folding, meaning they incorporate the effects of the detector response, before being compared to the observed data. By adjusting the model parameters, the predicted event counts in each bin can be varied to find the \emph{best fit} to the data. The number of observed events $n$, in each bin $i$ ($n_i$), is assumed to follow a Poisson distribution\sidenote{This is a good assumption because what we are doing here is essentially a counting experiment in every bin.} \sidecite{data_analysis_book}. The expected number of events in bin $i$, $\mu_i(\theta,\xi)$, represents the sum of contributions from all signal and background components. The Poisson likelihood for a given bin is defined as:

\begin{equation}\label{eq:Poisson}
    P(n_i|\mu_i(\theta,\xi)) = \frac{\mu_i(\theta,\xi)^{n_i} e^{-\mu_i(\theta,\xi)}}{n_i!}
\end{equation}

where $\mu_i(\theta,\xi)$ is the expected number of events in bin $i$ and $n_i$ is the observed number. The total likelihood for the entire dataset is then the product of the likelihoods for all bins:

\begin{equation}\label{eq:llh_noprior}
    \mathcal{L}(n|\mu_i(\theta,\xi)) = \prod_{i=1}^{N} P(n_i|\mu_i(\theta,\xi))
\end{equation}

Incorporating systematic uncertainties into the likelihood function is crucial for obtaining realistic results, as these uncertainties often arise from imperfect detector calibration or background estimations. Such systematic uncertainties are modeled as nuisance parameters in the likelihood fit. These parameters are typically constrained by prior knowledge and treated as additional variables in the fitting process. For each of the $k$ nuisance parameters, $\xi_j$, a Gaussian prior is introduced, penalizing deviations from the central value $\bar{\xi_j}$ by an amount proportional to the known uncertainty $\sigma_{\xi_j}$, extending the likelihood to:

\begin{equation}\label{eq:llh_withprior}
    \mathcal{L}(n|\mu_i(\theta,\xi)) = \prod_{i=1}^{N} P(n_i|\mu_i(\theta,\xi)) \prod_{j=1}^{k} \exp\left(-\frac{1}{2}\left(\frac{\xi_j - \bar{\xi_j}}{\sigma_{\xi_j}}\right)^2\right)
\end{equation}
By maximizing this likelihood, the best-fit parameters $\theta$ can be found. In practice, the fit is performed by minimizing $-\ln\mathcal{L}$, as it simplifies the computation (see \ref{eq:logllh}). 

\begin{equation}\label{eq:logllh}
    \ln(\mathcal{L}(n|\mu_i(\theta,\xi))) = \sum_{i=1}^{N} \left[ n_i \ln(\mu_i) - \ln(n_i!) - \mu_i \right] 
    - \sum_{j=1}^{k} \frac{1}{2} \left( \frac{\xi_j - \bar{\xi_j}}{\sigma_{\xi_j}} \right)^2
\end{equation}

Once the likelihood function is constructed, the next step is hypothesis testing. A specific model hypothesis can be tested by using a likelihood ratio test. The likelihood ratio compares two hypotheses: the test model (with parameters $\theta_t$) and the best-fit model (with parameters $\hat{\theta}$):

\begin{equation}\label{eq:llh_ratio_TS}
    -2 \Delta \ln \mathcal{L} = -2 \ln \left( \frac{\mathcal{L}(n|\mu(\theta_t, \xi_t))}{\mathcal{L}(n|\mu(\hat{\theta}, \hat{\xi}))} \right)
\end{equation}

The factor $-2$ is included for convenience, as the thus constructed test statistic follows a $\chi^2$ distribution under certain regularity conditions \sidecite{profile_llh}. Now, according to Wilks' theorem \sidecite{Wilks_thm}, this enables the calculation of confidence levels (CL) of the parameter of interest without the need for calculating the sampling distribution of $-2 \Delta \ln \mathcal{L}$ based on pseudo-experiments, which can be computationally expensive. However, Wilks’ theorem is only valid when the dataset is large and the model parameters are unbounded. If either condition is not met, the test statistic may deviate from a $\chi^2$-distribution, necessitating the use of Monte Carlo pseudo-experiments to determine the confidence intervals via the so-called \emph{Feldman Cousins} method \sidecite{feldman_cousin}, with an extension to incorporate nuisance parameters \sidecite{feldman_cousin_nuisance,asimov}.

The test statistic derived from the likelihood ratio, defined in Equation~\ref{eq:llh_ratio_TS} can be used to construct confidence regions for the parameters of interest, the process known as \textbf{a profile likelihood test}. For instance, in one-dimensional profile likelihood test, the confidence interval is obtained by comparing the test statistic to the cumulative distribution of a $\chi^2$-distribution with one degree of freedom. When scanning over two or more parameters, calculating the profile likelihood ratio on a multi-dimensional grid of parameter values is required, which significantly increases the computational complexity. In some cases, the Asimov dataset \cite{asimov}—a single dataset representing the median outcome of many pseudo experiments—can be used to estimate the median sensitivity of the analysis. This method provides an efficient way to estimate confidence intervals and test the sensitivity of the analysis without the need for exhaustive Monte Carlo pseudo-experiments. The Asimov dataset and Wilks' theorem are both used to derive the sensitivity of the analysis as presented in \ref{sec:sensitivty}. It is important to point out that the primary parameters of interest in this analysis are the astrophysical neutrino flavour fractions. 
The number of $\nu_{\tau}$ events, which provide the strongest constraint on astrophysical $\nu_{\tau}$ fraction in the flux, is small. Additionally, the flavour fractions in the likelihood fit are constrained to be non-negative always, therefore the applicability of Wilk's therom is evaluated as needed.

\subsection{SAY Likelihood}
\label{sec:SAY}

The likelihood construction described in section \ref{sec:MLE} relies on the Poisson likelihood, which assumes that the expectations for each bin, $\mu_i$, to be known exactly. This assumption holds when the Monte Carlo simulations used in generating the expected distributions have large statistics, so that the uncertainty on $\mu_i$ due to finite MC statistics is small compared to the uncertainty on the observed data counts. However, MC simulations inherently come with their own uncertainties due to finite statistics, particularly in highe energy bins where the difficulty to populate them becomes larger due to increase in computational costs.

In binned forward-folding fits, statistical inferences are made by comparing observed data counts in each bin to the expected counts derived from the simulations. As outlined in Equation~\ref{eq:Poisson}, the Poisson likelihood assumes that $\mu_i$, the expected value per bin, is well-determined. However, this is often not the case in analyses like the one performed in this thesis, where overall event rates are low, and individual bin expectations may fluctuate significantly due to limited MC statistics. This mismatch can lead to can lead to overconfident predictions if these fluctuations in $\mu_i$ are not correctly accounted for.

To address this issue, an extended form of the Poisson likelihood ($\mathcal{L}_{\mathrm{eff}}$), or the so-called \textbf{SAY likelihood}, is used in this analysis \sidecite{SAY}. This likelihood incorporates the uncertainty from the limited MC statistics and provides more appropriate coverage for parameter estimates based on this likelihood. Specifically, the distribution of MC event weights in each bin is modeled using a scaled Poisson distribution, which introduces an additional uncertainty term, $\sigma_j^2$, for each bin. This term depends on the weights $w_j$ of the simulated MC events $j$ in the $i$-th bin and is defined as:

\begin{equation}
\sigma_i^2 = \sum_{j} (w_j^i)^2
\end{equation}

This uncertainty term, $\sigma_i^2$, is larger in bins with low statistics, and smaller in bins with higher statistics, ensuring that fluctuations in low-stat bins are adequately covered. This formalism results in more conservative limits, with generally wider contours for the final result, as the additional statistical uncertainty is incorporated into the fit. One limitation of this approach is that the Asimov dataset \sidecite{asimov}, described in the previous section, cannot be constructed when using the SAY likelihood. This is because the $\sigma_j$ term can significantly skew the distribution $\mathcal{L}_{\text{eff}}(\mu_i | \sigma_i, d_i)$ so that the maximum is not at $\mu_i=d_i$, which is the defining property of the Asimov set. As a result, deviations from injected values may appear, especially in cases like the simulation produced for single atmospheric muons (\texttt{MuonGun}, explained in Section~\ref{sec:sim_ic}), which exhibits lack of high statsics due to high energy threshold of HESE sample, used in this work. Alternatively, the standard Poisson likelihood can be used, though the resulting limits may be overconfident.


\section{Components of the Likelihood Fit}
\label{sec:components}
The choice of the likelihood function and its subsequent requirements (the SAY likelihood)  was outlined in the previous section. In this section, the focus shall be on the essential components involved, and their modeling. The construction proceeds as follows:

\begin{enumerate}
    \item The parameters to be measured, known as \emph{signal parameters} (denoted by $\theta$), along with all \emph{nuisance parameters} ($\xi$), which may originate from uncertainties in the detector response or background modeling (such as atmospheric spectra), are identified and bounded where necessary.
    \marginnote{
        \begin{kaobox} 
        This is why a penalty term is introduced in the likelihood. A Gaussian prior prefers that the parameters remain within the $1\sigma$ prior region, penalizing the likelihood if the fit favors values outside this range. 
        \end{kaobox}}
    \item These parameters are applied in the form of the neutrino \emph{flux}, assigning each Monte Carlo event a specific \emph{weight} based on the individual fluxes.

    \item A set of observables that are most sensitive to the signal parameters, such as astrophysical neutrino fluxes, is selected and the simulated and experimentally observed events are accordingly binned for the analysis. The experiment is performed, and the observed data events $n$ are sorted into the predefined bins of the analysis histograms.
    \
    \item  A fit is conducted by varying each of the parameters (and, consequently, the fluxes) until the observed data events are optimally described. 
\end{enumerate}


\subsection{Model parameters}
\label{sec:params}
All the model parameters used in the fit, grouped by the flux component (astrophysical or atmopsheric) that they affect, are listed in \reftab{fit_params}. Moreover, several nuisance parameters describing uncertainty in the detector response are included in the fit. Baseline models and their corresponding values, wherever applicable, are specified for each parameter, along with the choice of priors. The astrophysical flux parameters represent the high-energy neutrino contributions from distant sources, while atmospheric flux models account for neutrinos originating from cosmic ray interactions within in atmosphere of Earth. 

\begin{table*}[h]
    \caption[The description of the parameters used in the likelihood fit, along with priors (if used)]{Parameters used in the likelihood desribed in Equation \ref{eq:llh_withprior}. The gaussian priors on the parameters (if applicable) in terms of the mean ($\mu$) and width ($\sigma$) are stated on the alongside.}
    \labtab{fit_params}
    
    \begin{tabular}{ |c |c|}
        \hline
        Parameter & Prior ($\mu,\sigma$)\\
        \hline
        \hline
        \multicolumn{2}{c}{\textbf{Astrophysical Flux (Signal) Parameters}}\\
        \hline
        astro. $\nu_{\mu}$ normalisation [$10^{-18} \mathrm{GeV}^{-1}\mathrm{cm}^{-2}\mathrm{sr}^{-1}\mathrm{s}^{-1}$]($\phi_{\nu_{\mu}}$) & - \\
        % \hline
        astro. spectral index ($\gamma_{\mathrm{astro}}$) & -\\
        % \hline
        scaling factor to modify total flux norm ($\Phi_{\nu+\bar\nu}^{\mathrm{astro}}$) relative to $\phi_{\nu_{\mu}}$ ($s_{\nu_e}$ astro. $\nu_{e}$)&- \\
        % \hline
        astro. $\nu_{\tau}$ scaling factor to modify total flux norm ($\Phi_{\nu+\bar\nu}^{\mathrm{astro}}$) relative to $\phi_{\nu_{\mu}}$ ($s_{\nu_{\tau}}$) & -\\
        \hline
        \multicolumn{2}{c}{\textbf{Atmospheric Flux Systematics}}\\
        \hline
        Conventional Flux normalisation ($\Phi_{\mathrm{conv}}$)  & (1.0,0.2)\\
        % \hline
        Prompt Flux Normalisation ($\Phi_{\mathrm{prompt}}$) &  - \\
        % \hline
        Interpolation between Cosmic Ray Models ($\xi_{\mathrm{CR}}$)  & (0,1)\\
        % \hline
        Cosmic Ray Spectral Index Shift ($\Delta\gamma_{\mathrm{CR}}$) &  (0,0.05)\\
        % \hline
        Barr-parameter modifying the pion-contribution ($H_{\mathrm{Barr}}$) & (0,0.15)\\
        % \hline
        Barr-parameter modifying the kaon-contribution ($W_{\mathrm{Barr}}$) & (0,0.40)\\
        % \hline
        Barr-parameter modifying the kaon-contribution ($Y_{\mathrm{Barr}}$)& (0,0.30)\\
        % \hline
        Barr-parameter modifying the kaon-contribution ($Z_{\mathrm{Barr}}$) & (0,0.12)\\
        % \hline
        Muon Flux Normalisation ($\Phi_{\mathrm{muongun}}$)  & (1,0.5)\\
        % \hline
        Scale factor for Neutrino Nucleon Inelasticity weight ($\mathrm{I}_{\mathrm{scale}}$) & (1,0.1)\\
        \hline
        
        \multicolumn{2}{c}{\textbf{Detector Systematics}}\\
        \hline
        Optical Efficiency of DOMs ($\eta_{\mathrm{domeff}}$) & (1.0,0.1)\\
        % \hline
        Ice Absorption Scaling ($\eta_{\mathrm{abs}}$)  & (1.0,0.05)\\
        % \hline
        Ice Scattering Scaling ($\eta_{\mathrm{scat}}$) & (1.0,0.05)\\
        % \hline
        Parametrization for refrozen icecolumn ($\eta_{\mathrm{h.ice-p0}}$)  & (-0.27,0.5)\\
        % \hline
        Parametrization for refrozen icecolumn ($\eta_{\mathrm{h.ice-p1}}$) & (-0.042,0.05)\\
        % \hline
        Ice Anisotropy Scaling ($\eta_{\mathrm{aniso}}$) &  -\\
        \hline
        \hline
    \end{tabular}
    
\end{table*}

\textbf{The flux of astrophysical neutrinos} is modeled as a single power law of the form
\begin{equation}\label{eq:SPL}
\Phi_{\nu + \bar{\nu}}^{\mathrm{astro}}(E_{\nu}) = \Phi_{\mathrm{0}}^{\mathrm{astro}} \times 
    (1 + s_{\nu_{e}} + s_{\nu_{\tau}}) \times
    \left( \frac{E_\nu}{\mathrm{100\,TeV}} \right)^{-\gamma_{\mathrm{astro}}}
\end{equation}
where $\gamma_{\mathrm{astro}}$ is the spectral index, $E_\nu$ is the neutrino energy, and $\Phi_{\nu + \bar{\nu}}^{\mathrm{astro}}$ is the all-flavour (including particle and anti-particle) normalization. Two scaling factors, $s_{\nu_{e}}$ and $s_{\nu_{\tau}}$, modify the flux of electron and tau neutrinos relative to the muon neutrino flux $\Phi_{\mathrm{0}}$. These scaling factors relate to the flavour ratio $f_{\nu_e}:f_{\nu_{\mu}}:f_{\nu_{\tau}}$ as:

\begin{equation}\label{eq:flav_frac}
    \begin{array}{rcl}
        f_{e} = \frac{s_{\nu_{e}}}{(1 + s_{\nu_{e}} + s_{\nu_{\tau}})}, \quad
        f_{\mu} = \frac{1}{(1 + s_{\nu_{e}} + s_{\nu_{\tau}})}, \quad
        f_{\tau} = \frac{s_{\nu_{\tau}}}{(1 + s_{\nu_{e}} + s_{\nu_{\tau}})}
    \end{array}
\end{equation}

The signal parameters for the flavour analysis are these two scaling factors, $\Phi_{\mathrm{0}}$, and $\gamma_{\mathrm{astro}}$. All other subsequent parameters explained are the nuisance parameters. In Eq.~\ref{eq:SPL}, it is assumed that each flavour has an identical spectral shape ($\gamma_{\mathrm{astro}}$). This assumption is justified because the HESE sample used in this analysis is small and has limited power to constrain spectral properties resolved by neutrino flavour. For the sensitivity study described in \ref{sec:sensitivity}, the benchmark spectral parameter values are taken from the previous iteration of this analysis \sidecite{HESE7_sample}, with $\gamma_{\mathrm{astro}} = 2.87$ and $\Phi_{\mathrm{0}} = 2.12 \times 10^{-18} \mathrm{GeV}^{-1}\ \mathrm{cm}^{-2}\ \mathrm{sr}^{-1}\ \mathrm{s}^{-1}$ assuming an equipartition of flavour ratios (i.e., $\nu_e : \nu_{\mu} : \nu_{\tau} = 1:1:1$).

\textbf{Atmospheric flux components} are constrained by priors because they are more accurately measured with other IceCube data samples. In contrast, the number of atmospheric events in the HESE selection is low due to the high efficiency in reducing atmospheric muons due to the veto requirement and the down-going atmospheric neutrinos from the same cosmic-ray-induced air shower, thanks to the self-veto effect.

Depending on the parent hadron type, the atmospheric neutrino flux is divided into two components: \textbf{The Conventional neutrino flux} (produced in the decay of charged pions and kaons) and \textbf{The Prompt Neutrino Flux} (produced in the decay of charmed mesons). The various models of atmospheric neutrino fluxes are detailed in \ref{sec:atm_nu}. The atmospheric neutrino flux contribution is modeled using the Matrix Cascade Equation Solver (MCEq) \sidecite{MCEq}. The baseline primary cosmic-ray model for this calculation is H4a \sidecite{gaisser_H4a}, combined with a hadronic interaction model \texttt{SIBYLL2.3c} \sidecite{SIBYLL23c}, introducing conventional ($\Phi_{\mathrm{conv}}$) and prompt ($\Phi_{\mathrm{prompt}}$) flux normalizations to account foverall flux level uncertainties. Additionally, they are folded with the generalized self-veto probability, as described in \ref{sec:HESE}, to account for the reduced rate of atmospheric neutrinos accompanying vetoed muons from the same cosmic-ray-induced air shower. Other nuisance parameters modifying the shape of these spectra include $\xi_{\mathrm{CR}}$, which interpolates between two primary cosmic-ray models H4a and GST, an approach introduced in \sidecite{diffusenumu}. The cosmic-ray model uncertainty is the largest source of uncertainty in the overall atmospheric background. Furthermore, $\Delta\gamma_{\mathrm{CR}}$ accounts for corrections in the spectral index of the primary cosmic-ray spectrum, see for e.g. \sidecite{hans_thesis}. Uncertainties in the hadronic interaction models, due to various $\pi$ and $K$ production cross-sections in different energy phases, introduce energy-dependent variations in atmospheric flux expectations \sidecite{Barr}. These nuisance parameters, referred to as \emph{Barr} parameters, are computed as described in \sidecite{Anatoli_HI_model}. A detailed discussion on implementing these parameters within the software framework can be found in \sidecite{richard_thesis}.

As mentioned, the veto and charge/energy cuts effectively reduce the rate of atmospheric neutrinos. However, high-energy single muons can still enter the detector, contaminating the sample. \textbf{The atmospheric Muon Flux} component is constrained by a prior partially derived from experimental data. The spectral shape is determined using a dedicated simulation that produces a flux of single muons reaching the detector (\texttt{MUONGUN}, described in \ref{sec:mc_sim}), weighted assuming the same baseline models used for atmospheric spectra (H4a). A pure-proton composition is chosen for these muons as it contributes the highest energy single muons. The normalization is estimated using the tagging method described in \cite{HESE7_sample}. A separate nuisance parameter, $\Phi_{\mathrm{muongun}}$, is included in the fit to account for these muons. A prior derived from this study is applied in this analysis. The sensitive energy range of this analysis is high, and the sample is specifically designed to minimize background contamination from atmospheric neutrinos and muons (see Section \ref{sec:HESE}). The available \texttt{MUONGUN} Monte Carlo was generated for various other IceCube analyses, that targeted lower energies as well, where background contributions are significantly higher. Most Monte Carlo events do not pass the strict cuts of this analysis, with no events at all in the cascade and double cascade bins (see \reffig{cascade_mc} and \reffig{double_mc}). Consequently, the statistical uncertainty on this component is high, requiring a Kernel Density Estimate (KDE) for smoothing.

The remaining nuisance parameters are those arising from the imperfect knowledge of the detector response, such as the Digital Optical Modules (DOMs, see section \ref{sec:DOM}), and the detection medium, which includes both the bulk ice and hole ice models (see section \ref{sec:icemodel}), denoted as from here-on as \textbf{Detector Systematics}. These detector systematics introduce uncertainties that can affect both the energy scale of the neutrino events and the shape of observable distributions, thereby altering the expected event rates from different flux components. Unlike previous iterations of this analysis \sidecite{marcel_thesis,Juliana_thesis}, which relied on discrete Monte Carlo simulation sets with different values for systematics that significantly impacted the analysis observables, the current approach uses the SnowStorm simulation technique (see section \ref{sec:snowstorm}). The SnowStorm method \sidecite{snowstorm} captures the full range of detector systematic variations in a single Monte Carlo dataset, avoiding the need for generating multiple datasets. It uses a first-order Taylor expansion of the observables, such as event counts in each bin, with respect to the detector systematics. The resulting gradient, called \emph{the nuisance gradient} ($\vec{G}$), allows for efficient computation of observable values as these systematic parameters vary. The expectation value of per bin event count $\mu$, for nuisance parameter variations $\vec{\eta}$ is given as,

\begin{equation}\label{eq:gradient}
    \mu(\vec{\eta})  = \mu_{\mathrm{baseline}} + \vec{\eta}.\vec{G}
\end{equation}

where, $\mu_{\mathrm{baseline}}$ is the expectation value of per bin count at basleine values of all detector systematic parameters (baseline values are the central value at which simulation was generated), when $\vec{\eta}=\vec{0}$. For this approach to be valid, it assumes that systematic effects are small enough to be treated perturbatively, allowing for a linear approximation. By marginalizing over the full ensemble of systematic variations, the prediction converges to that of the central model, effectively neglecting second-order effects. Because the dataset itself have finite statistics, appropriate uncertainty propagation is applied to the nuisance gradient (see \sidecite{richard_thesis} for details of calculations of this gradient, their uncertainties and incorporation within such fits). 

For each analysis bin, both the expected number of events ($\mu_i$) and the associated variance ($\sigma_i$) are computed, that enter the likelihood calculation, described in Equation~\ref{eq:logllh}. However, two important caveats should be noted with this method. First, the gradients are calculated based on a specific flux assumption\sidenote{For the analysis presented in this thesis, the gradients are computed using the best-fit values from the HESE-7.5 year analysis \cite{HESE7_sample}, with $\gamma_{\mathrm{astro}}=2.87$, the per-flavour flux normalization $\Phi_{\nu + \bar{\nu}}^{\mathrm{astro}}=2.12$, and an equipartition among all three flavour fluxes.} and are not recalculated at every minimization step. This significantly reduces the computational time of the minimization process but may introduce inaccuracies if the assumed fluxes deviate significantly from the true values in nature, which could substantially alter the resulting gradients. A potential solution is to compute the gradients over a range of flux assumptions to check for significant deviations in the results. For the analysis presneted in this thesis, this was tested by varying single-power law paramters (index and normalisation) defined in Equation~\ref{eq:SPL}, in appropriate range\sidenote{The appropriate range here refers to a set of spectral measurements made by various IceCube samples \cite{diffusenumu,cscd_6yr,HESE7_sample,ESTES}.} and the results ensured that this was a higher order effect. The second caveat arises because the expected event counts per bin, calculated using the gradients as shown in Equation \ref{eq:gradient}, are applied as an \emph{additive} correction to the overall flux expectation. Since the gradient is a real valued vector, it can also take negative values. For softer spectra and the limited statistics of the HESE sample, this can result in negative bin counts when the expected values approach zero. This presents a problem for the likelihood fit used in the analysis, as the logarithm of a negative number\sidenote{also, a negative number of expected events is unphysical anyway!} cannot be computed (which the fitter will attempt to do at some point; see Equation~\ref{eq:logllh}). To avoid this numerical issue, the expectation value calculated via the gradients is clipped at $\mu = 0 + \epsilon$. This ad hoc correction is justified because, although $\mu_{\mathrm{baseline}}$ may be zero in these bins, there remains significant statistical uncertainty, which typically dominates over the systematic variations.

The aforementioned gradient, can be constructed using 6 detector systematics simulated within the snowstrom ensemble. For the analysis presented in this thesis, it is constructed using 6 parameters, listed in the \reftab{fit_params}. 
\begin{description}
    \item [\textbf{The DOM Efficiency ($\eta_{\mathrm{domeff}}$)}] accounts for factors like the quantum efficiency of the PMT and shadowing by attached cables, reducing the efficiency by 1\%. The overall uncertainty is estimated at $\pm10\%$ \sidecite{spicemie}. This impacts the reconstructed energy, as lower efficiency underestimates the photon light yield. This effect is relevant in the analysis, as it shifts the energy distribution across bins and the energy thresholds in particular.
    \item [\textbf{Absorption ($\eta_{\mathrm{abs}}$) and Scattering ($\eta_{\mathrm{scat}}$) Scaling}] accounts for variations in scattering and absorprion coefficients of the ice model, mapped across a depth grid of the south pole ice. The baseline model for this mapping is SPICE-3.2.1, with priors set to a 5\% width around the central value, based on the uncertainty derived from calibration data obtained using flasher LEDs \cite{spicemie}. An increase in scattering coefficients extends the photon scattering path, increasing the chance of absorption before detection. Variations in ice scattering and absorption mainly affect energy reconstruction and may introduce minor directional bias, with the energy shift being the primary concern in this analysis.  
    \item [\textbf{hole ice parameters ($\eta_{\mathrm{h.ice-p0}}$ and $\eta_{\mathrm{h.ice-p1}}$)}] accounts for the differences in optical properties between the refrozen ice in the DOM deployment columns and the surrounding bulk ice. Uncertainties and priors are based on a recent study that analyzed flasher data \sidecite{holeice_eller_icrc23}.
    \item [\textbf{Ice anistropy}] discussed in detail in sections \ref{sec:icemodel} and \ref{sec:icemodel_checks} affects the double cascade reconstruction greatly. For the ice model used in simulation, this parameter is modelled as a modulation of the nominal scattering coefficient, analogous to that used in \sidecite{Juliana_paper}.
\end{description}



Lastly, an additional nuisance parameter, affecting the DIS neutrino interaction cross section (discussed in Section~\ref{sec:DIS}) and thus all the neutrino fluxes (Astrophysical and atmospheric) is used. The so-called \textbf{Neutrino-Nucleon Inelasticity Scaling ($\mathrm{I}_{\mathrm{scale}}$)} parameter is introduced to account for  inelasticity which is a measure of the fraction of the neutrino's energy that is transferred to the hadronic system in an inelastic neutrino nucleon scattering process, mathematically given as,
\begin{equation}
     y  = \frac{E_{\text{hadron}}}{E_\nu}
\end{equation}
The Mean inelasticity $\langle y \rangle$ is the average value of the inelasticity $y$ over many scattering events. It provides a statistical measure of the average fraction of the neutrino's energy that is transferred to the hadronic system. This is particularly relevant for starting tracks, as the mean inelasticity, \(\langle y \rangle\), depends on the incoming neutrino energy. Since \(\langle y \rangle\) evolves as a function of energy (see Section~\ref{sec:inelasticity}), it indicates the fraction of the neutrino's energy transferred to the hadronic system during the \(\nu_{\mu}\) charged-current (CC) interaction. Consequently, this also determines how much energy is imparted to the muon in starting tracks. Thus, variations in \(\langle y \rangle\) with energy influence the overall energy distribution of the events. For this analysis, a scaling factor is introduced, which is based on formulation provided in \sidecite{gary_paper}, that allows to vary Normalisation $N$ of the inelasticity distribution, 
\begin{equation}
    \frac{dp}{dy} = N \left( 1 + \epsilon (1 - y)^2 \right) y^{\lambda - 1}
\end{equation}
where N is the normalisation given as,
\begin{equation}\label{eq:inel}
    N = \frac{\lambda (\lambda + 1) (\lambda + 2)}{2\epsilon + (\lambda + 1) (\lambda + 2)}
\end{equation}

The theoretical prediction of \(\langle y \rangle\) from the CSMS model \sidecite{CSMS} is calculated for the mean inelasticity distribution for a given event. By fixing \(\langle y \rangle\), $\lambda$ \ref{eq:inel} is the fitted to apply a reweighting factor to \textit{scale} the inelasticity ($\mathrm{I}_{\mathrm{scale}}$). Variations are permitted within 10\% of their default value. The parameter is applied to all neutrino fluxes as via Deep Inelastic Scattering.

\subsection{Analysis Observables and their Distributions}
\label{sec:hists}

In forward folding fits, selecting appropriate observables is crucial for distinguishing between the signal and background hypotheses. The analysis uses all the HESE events above 60 TeV, further divided in 3 sub-samples of \emph{Single Cascades, Tracks and Double Cascades}. These 3 sub-samples are binned in 2-dimensional Monte Carlo templates of appropriate observables, such that bakground and signal templates are clearly distingishabe. When measuring the spectrum of astrophysical neutrinos, the background consists of neutrinos produced in atmospheric air showers. Key observables that allow for effective differentiation between astrophysical and atmospheric neutrinos include the total deposited energy ($\mathrm{E}_\mathrm{tot}$) and the zenith angle ($\theta_{\mathrm{zenith}}$) of the interacting neutrino. The motivation for focusing on these variables stems from their distinct behavior in atmospheric and astrophysical neutrino spectra (see Section~\ref{sec:atm_nu}). In particular, down-going atmospheric neutrinos are efficiently suppressed due to the self-veto effect, while up-going atmospheric neutrinos, which cannot be reduced by this veto, become an irreducible background. 

\marginnote{
        \begin{kaobox} 
        The astrophysical spectrum that is assumed for all the Monte Carlo templates and sensitivity is using a single-power law based on best-fit values of HESE-7.5 years spectrum measurements \cite{HESE7_sample}, given as,
            \begin{equation}\label{eq:SPL}
                \frac{d\Phi_{\nu + \bar{\nu}}^{\mathrm{astro}}}{dE} = 6.98.10^{-18}\left( \frac{E_{\nu}}{100 \, \text{TeV}} \right)^{-2.87} 
            \end{equation}
        In units of $\text{GeV}^{-1} \, \text{cm}^{-2} \, \text{s}^{-1} \, \text{sr}^{-1}$, with flavour composition of $\nu_e:\nu_{\mu}:\nu_{\tau}=1:1:1$
        \newline
        The prompt component is missing from all templates as best-fit value of $\Phi_{\mathrm{prompt}}$ of prompt flux normalization was 0. 
        \end{kaobox}
}
For the analysis presented in this thesis, the primary goal is to measure the flavour composition of astrophysical neutrinos. A key component of this analysis, as highlighted in Section~\ref{sec:PID}, is the identification and classification of $\nu_\tau$-induced double cascade events. Note that the PID assigned is not used as a proxy to tag flavour on an individual event bases, but through statistical analyses of the overall data set, Monte Carlo templates. Double cascades are a signature of $\nu_\tau$ interactions, and their identification relies heavily on the observables $\mathrm{E}_\mathrm{tot}$ and the reconstructed double cascade length ($\mathrm{L}_{\text{dc}}$), which provide the most discriminatory power for separating the flavours of astrophysical neutrinos, see \reffig{LvsE_signalbkg}. All double cascade events are binned into a two-dimensional Monte Carlo template, with one axis representing the reconstructed tau decay length ($L_{\text{dc}}$) and the other representing the reconstructed total deposited energy ($E_{\text{Tot}}$). The $L_{\text{dc}}$ is divided into 10 bins ranging from 10 m to 1000 m, while $E_{\text{Tot}}$ is divided into 13 bins spanning 60 TeV to 12.6 PeV, both using logarithmic spacing, as shown in ~\reffig{LvsE_signalbkg}. It is important to note that adding a third dimension, such as energy asymmetry, offers significant discriminatory power by reducing the low-length single cascade background, enhancing the sensitivity to measure the flavour ratio \sidecite{Neha_ICRC_IC}. However, a detailed study revealed that the limited Monte Carlo statistics and small small size of the HESE sample resulted in many empty bins, which posed challenges for forward-folding fits. As a result, the third dimension was ultimately excluded from the analysis.


\begin{figure*}[h!]
    \begin{subfigure}[h]{0.72\textwidth}
        \includegraphics{./figures/Analysis/LvsE_signal.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}[h]{0.72\textwidth}
        \includegraphics{./figures/Analysis/LvsE_Bkg.pdf}
       
    \end{subfigure}%
    \caption[Correlation of total deposited energy and length in 2D MC PDF, for signal and background double cascades]{2D Monte Carlo templates, constructed using reconstructed total energy ($E_{\text{Tot}}$) and double cascade length ($L_{\text{dc}}$) for events classified as \textbf{double cascades}. The signal (left), representing $\nu_\tau$-induced double cascades, shows a clear correlation between $L_{\text{dc}}$ and $E_{\text{tot}}$, with 68\% of events within the indicated signal region (dotted white line). In contrast, the background (right), consisting of $\nu_\mu$ and $\nu_e$ events, lacks this correlation and clusters at low $L_{\text{dc}}$, 68\% of all the background events lying below the indicated white dotted verticle line.}
    \labfig{LvsE_signalbkg}
\end{figure*}

Under the double cascade hypothesis, true single cascades are typically reconstructed with a small $\mathrm{L}_{\mathrm{dc}}$ but with most of their energy concentrated in the first cascade (with the energy asymmetry $E_A \rightarrow 1$). Hence, misclassified single cascades tend to cluster around low $\mathrm{L}_{\mathrm{dc}}$ and low $\mathrm{E}_{\mathrm{Tot}}$, with the distribution of $\mathrm{L}_{\mathrm{dc}}$ falling off rapidly, as indicated by the verticle line, on right panel of \reffig{LvsE_signalbkg}. In cases where a track is reconstructed under the double cascade hypothesis, the largest energy depositions are interpreted as the cascade vertices, leading to an arbitrary $\mathrm{L}_{\mathrm{dc}}$ value. These tracks, when misclassified as double cascades, typically exhibit low $\mathrm{E}_{\mathrm{Tot}}$, consistent with the falling astrophysical spectrum. This is also visible in right panel of \reffig{LvsE_signalbkg}.

True double cascades from $\nu_\tau$-CC interactions, on the other hand, exhibit a strong correlation between $\mathrm{L}_{\mathrm{dc}}$ and $\mathrm{E}_{\mathrm{Tot}}$. This correlation between $\mathrm{L}_{\mathrm{dc}}$ and $\mathrm{E}_{\mathrm{Tot}}$ is primarily used to determine the compatibility of an event with a $\nu_\tau$ interaction as opposed to another flavour. The Monte Carlo distributions (PDFs) of $\nu_\tau$-induced double cascades clearly show this correlation (see left panel of \reffig{LvsE_signalbkg}, indicating 68\% region of all classified true double cascade events). 

For single cascades and tracks, the observables $\mathrm{E}_{\mathrm{Tot}}$ (21 bins from 60TeV to 12.6 PeV in log space) and $\cos(\theta_z)$ (10 bins from -1 to 1 in cosine space) are commonly used, as they offer the most significant discrimination between astrophysical and atmospheric neutrinos, as shown in ~\reffig{cascades_2d} for cascades and ~\reffig{tracks_2d} for tracks. In both Figures, the supression of atmospheric neutrinos (right panel) ($\cos(\theta_z)$>0.25, the so-called \emph{down-going region} in IceCube) is clearly visible due to the self-veto effects, whereas astrophysical template (left panel) shows no such pattern\sidenote[-7cm]{Looking at the plot, it is not entirely true, as we do observe more events in the downgoing region $\cos{(\text{zenith})}\geq0.5$ because the up-going region requires a neutrino to travel much large distances through earth, and hence only a handful of neutrinos at high energies can do so.}, indicating neutrinos from all directions are accepted. 

\begin{figure*}[h!]
    \begin{subfigure}[h]{0.72\textwidth}
        \includegraphics{./figures/Analysis/Cascades_Astrophysical.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}[h]{0.72\textwidth}
        \includegraphics{./figures/Analysis/Cascades_Atmospheric.pdf}
       
    \end{subfigure}%
    \caption[2D Monte Carlo templates for cascade sample for astrophysical and atmopsheric fluxes]{2D Monte Carlo templates, constructed using reconstructed total energy ($E_{\text{Tot}}$) and reconstructed zenith ($\cos(\theta_z)$) for events classified as \textbf{single cascades}. The signal (left), representing \emph{Astrophysical neutrinos} of the sample and the background (right), representing \emph{Atmospheric neutrinos}, including conventional, prompt and single muon fluxes.}
    \labfig{cascades_2d}
\end{figure*}

\begin{figure*}[h!]
    \begin{subfigure}[h]{0.72\textwidth}
        \includegraphics{./figures/Analysis/Tracks_Astrophysical.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}[h]{0.72\textwidth}
        \includegraphics{./figures/Analysis/Tracks_Atmospheric.pdf}
       
    \end{subfigure}%
    \caption[2D Monte Carlo templates for tracks sample for astrophysical and atmopsheric fluxes]{2D Monte Carlo templates, constructed using reconstructed total energy ($E_{\text{Tot}}$) and reconstructed zenith ($\cos(\theta_z)$) for events classified as \textbf{tracks}. The signal (left), representing \emph{Astrophysical neutrinos} of the sample and the background (right), representing \emph{Atmospheric neutrinos}, including conventional, prompt and single muon fluxes.}
    \labfig{tracks_2d}
\end{figure*}

This becomes more evident in the one-dimensional (energy-averaged) distribution for both of these subsamples (\reffig{cascade_mc} for cascades and \reffig{tracks_mc} for tracks), where total expectation is broken down in individual flux contributions (as described in \ref{sec:params}). In the case of Energy distribution of single cascades (right panel of \reffig{cascade_mc}), except for lower energy bins (up to $\sim110$ TeV), astrophysical single cascade events dominate. The Glashow peak due to the resonant interaction of $\bar{\nu}_e$ is clearly visible too. Note the missing muon component, due to lack of enough \texttt{MUONGUN} simulation, as described in \ref{sec:params}. For Tracks (\reffig{tracks_mc}), there is a similar supression due to the self-veto effect. The constribution of single muons, simulated using \texttt{MUONGUN} events, is clearly visible, dominating in the same down-going region. As described before, the template is noisy due to large Monte Carlo uncertainties. In general, the tracks sample shows a larger contribution due to atmospheric fluxes compared to single cascades\sidenote{recall that single cascades also show contributions from all flavour Neutral Current (NC) interactions}. Similar one dimensional distributions of $\mathrm{L}_{\mathrm{dc}}$ and $\mathrm{E}_{\mathrm{Tot}}$ are shown for completeness in \reffig{double_mc}, but as described before, double cascades do not show any striking contributions from going the atmopsheric fluxes as the background for this sample is due to $\nu_{e}$ and $\nu_{\mu}$ events.


\begin{figure*}[h!]
    \caption[1D observable distribution of the total deposited energy and zenith angle for single cascades]{One-dimensional observable distribution, for HESE Single Cascades showing expected number of events as a function of reconstructed energy (right) and reconstructed zenith (left), broken down into different flux components, Astrophysical and Conventional Neutrinos and Muon (single muons). Only statistical errors of the MC simulation are shown.}
    \includegraphics{./figures/Analysis/Cascades.pdf}
    \labfig{cascade_mc}
\end{figure*}

\begin{figure*}[h!]
    \caption[1D observable distribution of the total deposited energy and zenith angle for tracks]{One-dimensional observable distribution, for HESE Tracks showing expected number of events as a function of reconstructed energy (right) and reconstructed zenith (left), broken down into different flux components, Astrophysical and Conventional Neutrinos and Muon (single muons). Only statistical errors of the MC simulation are shown.}
    \includegraphics{./figures/Analysis/Tracks.pdf}
    \labfig{tracks_mc}
\end{figure*}

\begin{figure*}[h!]
    \caption[1D observable distribution of the total deposited energy and reconstructed tau decay length for double cascades]{One-dimensional observable distribution, for HESE Double Cascades showing expected number of events as a function of reconstructed energy (right) and reconstructed tau decay length (left), broken down into different flux components, Astrophysical and Conventional Neutrinos and Muon (single muons). Only statistical errors of the MC simulation are shown.}
    \includegraphics{./figures/Analysis/Double.pdf}
    \labfig{double_mc}
\end{figure*}

\section{Analysis Sensitivity}
\label{sec:sensitivty}
All fits and related calculations are carried out using a software toolkit called \textbf{\texttt{NNMFit}}. Developed within the IceCube collaboration, \texttt{NNMFit} has been applied in many other IceCube analyses. Essentially, the toolkit handles the statistical modeling required for forward-folding fits using high-energy neutrino data with binned likelihoods. It supports Monte Carlo event weighting, testing various flux models using signal and nuisance parameters, applying systematic detector uncertainties, and performing fits to data. The toolkit also enables joint fitting across different event selections. The \texttt{aesara} backend \sidecite{aesera} allows fast and efficient forward folding of Monte Carlo samples, even for large datasets, and offers automated differentiation that greatly assists in optimizing likelihood with gradient-aware minimizers \sidecite{LBFGSB}. More details on this toolkit can be found in \sidecite{richard_thesis,erik_thesis}. 

Using all the ingredients constructed from signal and nuisance parameters, and following the described method, the flavour composition parameter space is scanned to obtain a two-dimensional confidence region for the flavour composition, as shown in \reffig{sensitivity}. To derive these limits, an Asimov dataset is constructed (see Section~\ref{sec:analysis}), assuming the benchmark astrophysical neutrino spectrum given in Equation~\ref{eq:SPL}. All other nuisance parameters are assumed to be at their baseline values listed in \reftab{fit_params}. The astrophysical neutrino flavour composition is constrained by evaluating the likelihood ratio in a profile likelihood scan, with confidence regions estimated using Wilks’s theorem. The expected number of events over 12 years of HESE data, assuming the spectrum in \ref{eq:SPL}, is broken down by flux components and shown in \reftab{expected_events}. The fitting procedure bins all HESE events above 60 TeV in reconstructed energy and further categorizes them into three subsamples based on morphology: single cascades, tracks, and double cascades. The three samples are fit jointly using the respective two-dimensional Monte Carlo templates, with the appropriate analysis variables shown in \reffig{cascades_2d}, \reffig{tracks_2d}, and \reffig{LvsE_signalbkg}.

\begin{table}[h]
    \caption[The expected number of events in 12 years of HESE data]{The expected number of events from different flux components in the HESE sample, assuming a livetime of $\sim 12$ years, for single cascades, double cascades, and tracks categories. Only Monte Carlo uncertainties are included. The astrophysical spectrum assumed follows Equation~\ref{eq:SPL}.}
    \labtab{fit_params}
    
    \begin{tabular}{ c |c|c|c}
        
        \hline
        &Single Cascades &Double Cascades&Tracks\\
        \hline
        \hline
        Astrophysical&$67\pm1$& $4\pm0.2$ & $13\pm0.5$\\
        Conventional & $5\pm0.7$& $0.2\pm0.1$ &$5\pm0.6$\\
        Atm. Muons & - & - & $2\pm3$\\
        \hline
        MC Sum & $72\pm2$ & $4\pm0.3$ & $20\pm3$\\
        \hline
    \end{tabular}
\end{table}

\begin{figure}[h!]
    \caption[Sensitivity to the astrophysical neutrino flavour composiition using 12 years of HESE data]{Sensitivity of the analysis presented in this thesis to measure the the flavour composition using $\sim12$ years of IceCube HESE data. A single power law given in Equation~\ref{eq:SPL} is assumed, with flavour composition of $\nu_e:\nu_{\mu}:\nu_{\tau}=1:1:1$. Contours show the $1\sigma$ (solid) and $2\sigma$ (dashed) confidence intervals assuming Wilks' theorem.}
    \includegraphics{./figures/Analysis/Asimov_Sensitivity.pdf}
    \labfig{sensitivity}
\end{figure}

The sensitivity results in \reffig{sensitivity} reveal that none of the standard source scenarios discussed in Section~\ref{sec:flavour_theory} can be rejected with high significance. For instance, the neutron beam scenario (represented by the dark blue triangle) is barely excluded at the $\sim1\sigma$ level. It is important to recall that the sensitivity of this analysis to the astrophysical tau-neutrino flux depends strongly on the assumed spectral shape of the neutrino flux. This sensitivity arises from the energy-dependent identification efficiency of tau-neutrino interactions. In interactions at higher neutrino energies, more energy is transferred to the secondary tau, increasing its decay length and enhancing identification efficiency. A softer spectrum, however, leads to more tau-neutrino interactions at lower energies, where they are harder to identify. Additionally, the sudden disappearance of line segments in the 95\% confidence contours, particularly in regions where the $\nu_\mu$ fraction approaches zero, is a result of the flavour fit parameterization (see Equation \ref{eq:flav_frac}). By construction, this fraction cannot be zero, meaning that this phase space is inaccessible to the fit\footnote{The \texttt{NNMFit} toolkit used in this analysis was initially developed for a spectrum measurement utilizing a sample of through-going tracks \cite{diffusenumu}. This sample, primarily consisting of muon neutrinos, was specifically designed to measure a non-zero \(\nu_{\mu}\) fraction, which informed the choice of flavour fraction parameterization. It is important to note, however, that alternative parameterization methods exist, where the flavour fractions of the three neutrino types are represented as a vector in three-dimensional space, with their relationships defined through a spherical coordinate transformation \cite{golemflavor}. This approach could also be implemented in \texttt{NNMFit}.}.

While earlier versions of this analysis explored alternative flux models (for example, a broken power law introduces a break in the conventional single power law, resulting in different spectral indices on either side of the break energy), this was not done here. The decision was based on recent findings from another IceCube analysis that combined two other event samples (track and cascade events, including much lower energies compared to HESE sample), revealing features in the extragalactic astrophysical neutrino spectrum based on the sensitivity to a larger range of astrophysical neutrino energies ($\sim10$ TeV to $\sim10$ PeV) \sidecite{globalfit_icrc}. That study provided evidence of a spectral break at around 24 TeV with more than $4\sigma$ significance. The spectrum showed hardness below this break, followed by softening at higher energies, consistent with the softer spectral measurement from HESE-7.5, which is sensitive to the astrophysical neutrino flux above $\sim 70$ TeV \sidecite{HESE7_sample}. Importantly, this high-statistics analysis, which extends well beyond the energy range of the current analysis, found no significant structures in the energy range accessible with the HESE sample. Furthermore, an independent study using a different event sample \sidecite{MESE_ICRC} reported similar spectral features and closely aligned best-fit values, strengthening the argument and above choice. 

